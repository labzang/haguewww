"""
ğŸ˜ğŸ˜ chat_service.py ì„œë¹™ ê´€ë ¨ ì„œë¹„ìŠ¤

ë‹¨ìˆœ ì±„íŒ…/ëŒ€í™”í˜• LLM ì¸í„°í˜ì´ìŠ¤.

ì„¸ì…˜ë³„ íˆìŠ¤í† ë¦¬ ê´€ë¦¬, ìš”ì•½, í† í° ì ˆì•½ ì „ëµ ë“±.

QLoRA (4-bit Quantized LoRA)ë¥¼ ì‚¬ìš©í•œ ëŒ€í™” ë° í•™ìŠµ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
"""
import os
from pathlib import Path
from typing import Optional, List, Dict, Any

import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
    DataCollatorForLanguageModeling,
)
from peft import (
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training,
    PeftModel,
    TaskType,
)
from trl import SFTTrainer
from datasets import Dataset, load_dataset


class QLoRAChatService:
    """QLoRAë¥¼ ì‚¬ìš©í•œ ëŒ€í™” ë° í•™ìŠµ ì„œë¹„ìŠ¤."""

    def __init__(
        self,
        model_name_or_path: str,
        *,
        adapter_path: Optional[str] = None,
        device_map: str = "auto",
        use_4bit: bool = True,
        lora_r: int = 64,
        lora_alpha: int = 16,
        lora_dropout: float = 0.1,
        target_modules: Optional[List[str]] = None,
    ):
        """QLoRA ì±„íŒ… ì„œë¹„ìŠ¤ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

        Args:
            model_name_or_path: ëª¨ë¸ ì´ë¦„ ë˜ëŠ” ê²½ë¡œ (Hugging Face ëª¨ë¸ ID ë˜ëŠ” ë¡œì»¬ ê²½ë¡œ).
            adapter_path: ê¸°ì¡´ LoRA ì–´ëŒ‘í„° ê²½ë¡œ (ì„ íƒì‚¬í•­).
            device_map: ë””ë°”ì´ìŠ¤ ë§¤í•‘ ("auto", "cuda", "cpu").
            use_4bit: 4-bit ì–‘ìí™” ì‚¬ìš© ì—¬ë¶€.
            lora_r: LoRA rank.
            lora_alpha: LoRA alpha.
            lora_dropout: LoRA dropout.
            target_modules: LoRAë¥¼ ì ìš©í•  ëª¨ë“ˆ ëª©ë¡ (Noneì´ë©´ ìë™ ê°ì§€).
        """
        self.model_name_or_path = model_name_or_path
        self.adapter_path = adapter_path
        self.device_map = device_map
        self.use_4bit = use_4bit
        self.lora_r = lora_r
        self.lora_alpha = lora_alpha
        self.lora_dropout = lora_dropout
        self.target_modules = target_modules

        self.model: Optional[AutoModelForCausalLM] = None
        self.tokenizer: Optional[AutoTokenizer] = None
        self.peft_model: Optional[PeftModel] = None

        self._load_model()

    def _load_model(self) -> None:
        """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        print(f"ğŸ”§ QLoRA ëª¨ë¸ ë¡œë”© ì¤‘: {self.model_name_or_path}")

        # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ğŸ–¥ï¸ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")

        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name_or_path)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id

        # 4-bit ì–‘ìí™” ì„¤ì •
        quantization_config = None
        if self.use_4bit and device == "cuda":
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4",
            )

        # ëª¨ë¸ ë¡œë“œ
        model_kwargs = {
            "device_map": self.device_map,
            "torch_dtype": torch.float16 if device == "cuda" else torch.float32,
            "trust_remote_code": True,
        }
        if quantization_config:
            model_kwargs["quantization_config"] = quantization_config

        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name_or_path, **model_kwargs
        )

        # ê¸°ì¡´ ì–´ëŒ‘í„°ê°€ ìˆìœ¼ë©´ ë¡œë“œ
        if self.adapter_path and Path(self.adapter_path).exists():
            print(f"ğŸ“¦ ê¸°ì¡´ LoRA ì–´ëŒ‘í„° ë¡œë”©: {self.adapter_path}")
            self.peft_model = PeftModel.from_pretrained(
                self.model, self.adapter_path
            )
            self.model = self.peft_model
        else:
            # ìƒˆ LoRA ì„¤ì •
            if self.target_modules is None:
                # ì¼ë°˜ì ì¸ í•œêµ­ì–´ ëª¨ë¸ì˜ íƒ€ê²Ÿ ëª¨ë“ˆ (ìë™ ê°ì§€ ì‹œë„)
                self.target_modules = ["q_proj", "k_proj", "v_proj", "o_proj"]
                if hasattr(self.model, "config"):
                    config = self.model.config
                    if hasattr(config, "model_type"):
                        model_type = config.model_type.lower()
                        if "llama" in model_type or "mistral" in model_type:
                            self.target_modules = ["q_proj", "k_proj", "v_proj", "o_proj"]
                        elif "gpt" in model_type:
                            self.target_modules = ["c_attn", "c_proj"]

            lora_config = LoraConfig(
                r=self.lora_r,
                lora_alpha=self.lora_alpha,
                target_modules=self.target_modules,
                lora_dropout=self.lora_dropout,
                bias="none",
                task_type=TaskType.CAUSAL_LM,
            )

            # í•™ìŠµ ì¤€ë¹„ (4-bit ëª¨ë¸ì¸ ê²½ìš°)
            if self.use_4bit:
                self.model = prepare_model_for_kbit_training(self.model)

            self.peft_model = get_peft_model(self.model, lora_config)
            self.model = self.peft_model
        print("ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜")
        print("âœ… QLoRA ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
        print("ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜")

    def chat(
        self,
        message: str,
        *,
        max_new_tokens: int = 512,
        temperature: float = 0.7,
        top_p: float = 0.9,
        do_sample: bool = True,
        conversation_history: Optional[List[Dict[str, str]]] = None,
    ) -> str:
        """ëŒ€í™”ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

        Args:
            message: ì‚¬ìš©ì ë©”ì‹œì§€.
            max_new_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜.
            temperature: ìƒì„± ì˜¨ë„.
            top_p: Top-p ìƒ˜í”Œë§.
            do_sample: ìƒ˜í”Œë§ ì‚¬ìš© ì—¬ë¶€.
            conversation_history: ëŒ€í™” íˆìŠ¤í† ë¦¬ (ì„ íƒì‚¬í•­).
                í˜•ì‹: [{"role": "user", "content": "..."}, {"role": "assistant", "content": "..."}]

        Returns:
            ìƒì„±ëœ ì‘ë‹µ í…ìŠ¤íŠ¸.
        """
        if self.model is None or self.tokenizer is None:
            raise RuntimeError("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        # ëŒ€í™” íˆìŠ¤í† ë¦¬ì™€ í˜„ì¬ ë©”ì‹œì§€ë¥¼ ê²°í•©
        if conversation_history:
            # íˆìŠ¤í† ë¦¬ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            formatted_messages = []
            for msg in conversation_history:
                role = msg.get("role", "user")
                content = msg.get("content", "")
                if role == "user":
                    formatted_messages.append(f"ì‚¬ìš©ì: {content}")
                elif role == "assistant":
                    formatted_messages.append(f"ì–´ì‹œìŠ¤í„´íŠ¸: {content}")
            formatted_messages.append(f"ì‚¬ìš©ì: {message}")
            formatted_messages.append("ì–´ì‹œìŠ¤í„´íŠ¸:")
            prompt = "\n".join(formatted_messages)
        else:
            prompt = f"ì‚¬ìš©ì: {message}\nì–´ì‹œìŠ¤í„´íŠ¸:"

        # í† í¬ë‚˜ì´ì§•
        inputs = self.tokenizer(
            prompt, return_tensors="pt", truncation=True, max_length=2048
        ).to(self.model.device)

        # ìƒì„±
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                top_p=top_p,
                do_sample=do_sample,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
            )

        # ë””ì½”ë”©
        generated_text = self.tokenizer.decode(
            outputs[0][inputs["input_ids"].shape[1] :], skip_special_tokens=True
        )

        return generated_text.strip()

    def train(
        self,
        dataset: Dataset,
        *,
        output_dir: str = "./checkpoints",
        num_train_epochs: int = 3,
        per_device_train_batch_size: int = 4,
        gradient_accumulation_steps: int = 4,
        learning_rate: float = 2e-4,
        warmup_steps: int = 100,
        logging_steps: int = 10,
        save_steps: int = 500,
        save_total_limit: int = 3,
        fp16: bool = True,
        dataset_text_field: str = "text",
    ) -> None:
        """QLoRAë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤.

        Args:
            dataset: í•™ìŠµ ë°ì´í„°ì…‹ (Hugging Face Dataset).
                í˜•ì‹: {"text": "instruction: ...\ninput: ...\noutput: ..."} ë˜ëŠ”
                      {"instruction": "...", "input": "...", "output": "..."}
            output_dir: ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ë””ë ‰í„°ë¦¬.
            num_train_epochs: í•™ìŠµ ì—í¬í¬ ìˆ˜.
            per_device_train_batch_size: ë””ë°”ì´ìŠ¤ë‹¹ ë°°ì¹˜ í¬ê¸°.
            gradient_accumulation_steps: ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ìŠ¤í… ìˆ˜.
            learning_rate: í•™ìŠµë¥ .
            warmup_steps: ì›Œë°ì—… ìŠ¤í… ìˆ˜.
            logging_steps: ë¡œê¹… ìŠ¤í… ê°„ê²©.
            save_steps: ì €ì¥ ìŠ¤í… ê°„ê²©.
            save_total_limit: ìœ ì§€í•  ì²´í¬í¬ì¸íŠ¸ ìˆ˜.
            fp16: FP16 ì‚¬ìš© ì—¬ë¶€.
            dataset_text_field: ë°ì´í„°ì…‹ì˜ í…ìŠ¤íŠ¸ í•„ë“œ ì´ë¦„.
        """
        if self.model is None or self.tokenizer is None:
            raise RuntimeError("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        print("ğŸš€ QLoRA í•™ìŠµ ì‹œì‘...")

        # ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ (í•„ìš”í•œ ê²½ìš°)
        if "text" not in dataset.column_names:
            # instruction-input-output í˜•ì‹ì„ textë¡œ ë³€í™˜
            def format_text(example: Dict[str, Any]) -> Dict[str, str]:
                instruction = example.get("instruction", "")
                input_text = example.get("input", "")
                output = example.get("output", "")

                if input_text:
                    text = f"### Instruction:\n{instruction}\n### Input:\n{input_text}\n### Response:\n{output}"
                else:
                    text = f"### Instruction:\n{instruction}\n### Response:\n{output}"

                return {"text": text}

            dataset = dataset.map(format_text)

        # í•™ìŠµ ì¸ì ì„¤ì •
        training_args = TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=num_train_epochs,
            per_device_train_batch_size=per_device_train_batch_size,
            gradient_accumulation_steps=gradient_accumulation_steps,
            learning_rate=learning_rate,
            warmup_steps=warmup_steps,
            logging_steps=logging_steps,
            save_steps=save_steps,
            save_total_limit=save_total_limit,
            fp16=fp16,
            optim="paged_adamw_32bit",
            lr_scheduler_type="cosine",
            report_to="none",
        )

        # ë°ì´í„° ì½œë ˆì´í„°
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer, mlm=False
        )

        # SFT Trainer ìƒì„±
        trainer = SFTTrainer(
            model=self.model,
            args=training_args,
            train_dataset=dataset,
            tokenizer=self.tokenizer,
            data_collator=data_collator,
            dataset_text_field=dataset_text_field,
            max_seq_length=2048,
            packing=False,
        )

        # í•™ìŠµ ì‹¤í–‰
        trainer.train()

        # ìµœì¢… ëª¨ë¸ ì €ì¥
        trainer.save_model()
        self.tokenizer.save_pretrained(output_dir)

        print(f"âœ… í•™ìŠµ ì™„ë£Œ! ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ìœ„ì¹˜: {output_dir}")

    def save_adapter(self, output_path: str) -> None:
        """LoRA ì–´ëŒ‘í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.

        Args:
            output_path: ì €ì¥ ê²½ë¡œ.
        """
        if self.peft_model is None:
            raise RuntimeError("PEFT ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")

        self.peft_model.save_pretrained(output_path)
        print(f"âœ… ì–´ëŒ‘í„° ì €ì¥ ì™„ë£Œ: {output_path}")


def create_qlora_chat_service(
    model_name_or_path: str,
    *,
    adapter_path: Optional[str] = None,
    **kwargs: Any,
) -> QLoRAChatService:
    """QLoRA ì±„íŒ… ì„œë¹„ìŠ¤ë¥¼ ìƒì„±í•˜ëŠ” íŒ©í† ë¦¬ í•¨ìˆ˜.

    Args:
        model_name_or_path: ëª¨ë¸ ì´ë¦„ ë˜ëŠ” ê²½ë¡œ.
        adapter_path: ê¸°ì¡´ LoRA ì–´ëŒ‘í„° ê²½ë¡œ (ì„ íƒì‚¬í•­).
        **kwargs: ì¶”ê°€ ì¸ì (QLoRAChatService.__init__ì— ì „ë‹¬).

    Returns:
        QLoRAChatService ì¸ìŠ¤í„´ìŠ¤.
    """
    return QLoRAChatService(model_name_or_path, adapter_path=adapter_path, **kwargs)
